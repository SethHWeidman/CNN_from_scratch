{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_net import conv_helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original', data_home=\".\")\n",
    "\n",
    "mnist_data = mnist.data\n",
    "mnist_target = mnist.target\n",
    "\n",
    "def show(image):\n",
    "    \"\"\"\n",
    "    Render a given numpy.uint8 2D array of pixel data.\n",
    "    \"\"\"\n",
    "    from matplotlib import pyplot\n",
    "    import matplotlib as mpl\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image, cmap=mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 63000\n",
    "\n",
    "X_train = np.reshape(mnist_data[:split], (-1, 1, 28, 28))/255.0\n",
    "y_train = mnist_target[:split]\n",
    "X_test = np.reshape(mnist_data[split:], (-1, 1, 28, 28))/255.0\n",
    "y_test = mnist_target[split:]\n",
    "n_classes = np.unique(y_train).size\n",
    "\n",
    "# Downsample training data\n",
    "n_train_samples = 3000\n",
    "train_idxs = np.random.randint(0, split-1, n_train_samples)\n",
    "X_train = X_train[train_idxs, ...]\n",
    "y_train = y_train[train_idxs, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels):\n",
    "    classes = np.unique(labels).astype(int)\n",
    "    n_classes = classes.size\n",
    "    one_hot_labels = np.zeros(labels.shape + (n_classes,))\n",
    "    for c in classes:\n",
    "        one_hot_labels[labels == c, c] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def unhot(one_hot_labels):\n",
    "    return np.argmax(one_hot_labels, axis=-1)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_d(x):\n",
    "    s = sigmoid(x)\n",
    "    return s*(1-s)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_d(x):\n",
    "    e = np.exp(2*x)\n",
    "    return (e-1)/(e+1)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "    dx = np.zeros(x.shape)\n",
    "    dx[x >= 0] = 1\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def _setup(self, input_shape, rng):\n",
    "        \"\"\" Setup layer with parameters that are unknown at __init__(). \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fprop(self, input):\n",
    "        \"\"\" Calculate layer output for given input (forward propagation). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        \"\"\" Calculate input gradient. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        \"\"\" Calculate shape of this layer's output.\n",
    "        input_shape[0] is the number of samples in the input.\n",
    "        input_shape[1:] is the shape of the feature.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class LossMixin(object):\n",
    "    def loss(self, output, output_pred):\n",
    "        \"\"\" Calculate mean loss given output and predicted output. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def input_grad(self, output, output_pred):\n",
    "        \"\"\" Calculate input gradient given output and predicted output. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ParamMixin(object):\n",
    "    def params(self):\n",
    "        \"\"\" Layer parameters. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def param_grads(self):\n",
    "        \"\"\" Get layer parameter gradients as calculated from bprop(). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def param_incs(self):\n",
    "        \"\"\" Get layer parameter steps as calculated from bprop(). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Linear(Layer, ParamMixin):\n",
    "    def __init__(self, n_out, weight_scale, weight_decay=0.0):\n",
    "        self.n_out = n_out\n",
    "        self.weight_scale = weight_scale\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def _setup(self, input_shape, rng):\n",
    "        n_input = input_shape[1]\n",
    "        W_shape = (n_input, self.n_out)\n",
    "        self.W = rng.normal(size=W_shape, scale=self.weight_scale)\n",
    "        self.b = np.zeros(self.n_out)\n",
    "\n",
    "    def fprop(self, input):\n",
    "        self.last_input = input\n",
    "        return np.dot(input, self.W) + self.b\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        n = output_grad.shape[0]\n",
    "        self.dW = np.dot(self.last_input.T, output_grad)/n - self.weight_decay*self.W\n",
    "        self.db = np.mean(output_grad, axis=0)\n",
    "        return np.dot(output_grad, self.W.T)\n",
    "\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    def param_incs(self):\n",
    "        return self.dW, self.db\n",
    "\n",
    "    def param_grads(self):\n",
    "        # undo weight decay to get gradient\n",
    "        gW = self.dW+self.weight_decay*self.W\n",
    "        return gW, self.db\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.n_out)\n",
    "\n",
    "\n",
    "class Activation(Layer):\n",
    "    def __init__(self, type):\n",
    "        if type == 'sigmoid':\n",
    "            self.fun = sigmoid\n",
    "            self.fun_d = sigmoid_d\n",
    "        elif type == 'relu':\n",
    "            self.fun = relu\n",
    "            self.fun_d = relu_d\n",
    "        elif type == 'tanh':\n",
    "            self.fun = tanh\n",
    "            self.fun_d = tanh_d\n",
    "        else:\n",
    "            raise ValueError('Invalid activation function.')\n",
    "\n",
    "    def fprop(self, input):\n",
    "        self.last_input = input\n",
    "        return self.fun(input)\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        return output_grad*self.fun_d(self.last_input)\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "class LogRegression(Layer, LossMixin):\n",
    "    \"\"\" Softmax layer with cross-entropy loss function. \"\"\"\n",
    "    def fprop(self, input):\n",
    "        e = np.exp(input - np.amax(input, axis=1, keepdims=True))\n",
    "        return e/np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        raise NotImplementedError(\n",
    "            'LogRegression does not support back-propagation of gradients. '\n",
    "            + 'It should occur only as the last layer of a NeuralNetwork.'\n",
    "        )\n",
    "\n",
    "    def input_grad(self, Y, Y_pred):\n",
    "        # Assumes one-hot encoding.\n",
    "        return -(Y - Y_pred)\n",
    "\n",
    "    def loss(self, Y, Y_pred):\n",
    "        # Assumes one-hot encoding.\n",
    "        eps = 1e-15\n",
    "        Y_pred = np.clip(Y_pred, eps, 1 - eps)\n",
    "        Y_pred /= Y_pred.sum(axis=1, keepdims=True)\n",
    "        loss = -np.sum(Y * np.log(Y_pred))\n",
    "        return loss / Y.shape[0]\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, rng=None):\n",
    "        self.layers = layers\n",
    "        if rng is None:\n",
    "            rng = np.random.RandomState()\n",
    "        self.rng = rng\n",
    "\n",
    "    def _setup(self, X, Y):\n",
    "        # Setup layers sequentially\n",
    "        next_shape = X.shape\n",
    "        for layer in self.layers:\n",
    "            layer._setup(next_shape, self.rng)\n",
    "            next_shape = layer.output_shape(next_shape)\n",
    "#            print(next_shape)\n",
    "        if next_shape != Y.shape:\n",
    "            raise ValueError('Output shape %s does not match Y %s'\n",
    "                             % (next_shape, Y.shape))\n",
    "\n",
    "    def fit(self, X, Y, learning_rate=0.1, max_iter=10, batch_size=64):\n",
    "        \"\"\" Train network on the given data. \"\"\"\n",
    "        n_samples = Y.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "        Y_one_hot = one_hot(Y)\n",
    "        self._setup(X, Y_one_hot)\n",
    "        iter = 0\n",
    "        # Stochastic gradient descent with mini-batches\n",
    "        while iter < max_iter:\n",
    "            iter += 1\n",
    "            for b in range(n_batches):\n",
    "                print(n_batches)\n",
    "                batch_time = time.time()\n",
    "                batch_begin = b*batch_size\n",
    "                batch_end = batch_begin+batch_size\n",
    "                X_batch = X[batch_begin:batch_end]\n",
    "                print(\"Shape of X_batch is:\",\n",
    "                      X_batch.shape)\n",
    "                Y_batch = Y_one_hot[batch_begin:batch_end]\n",
    "                print(\"Shape of Y_batch is:\",\n",
    "                      Y_batch.shape)\n",
    "\n",
    "                # Forward propagation\n",
    "                X_next = X_batch\n",
    "                for layer in self.layers:\n",
    "                    try:\n",
    "                        print(\"Shape of layer weights is:\", \n",
    "                              layer.W.shape)\n",
    "                    except:\n",
    "                        pass\n",
    "                    X_next = layer.fprop(X_next)\n",
    "                    print(\"Shape of layer output is:\", \n",
    "                          X_next.shape)\n",
    "                Y_pred = X_next\n",
    "\n",
    "                # Back propagation of partial derivatives\n",
    "                next_grad = self.layers[-1].input_grad(Y_batch, Y_pred)\n",
    "                for layer in reversed(self.layers[:-1]):\n",
    "                    print(\"Gradient passed from layer backwards has shape:\",\n",
    "                          np.array(next_grad).shape)\n",
    "                    next_grad = layer.bprop(next_grad)\n",
    "\n",
    "                # Update parameters\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, ParamMixin):\n",
    "                        for param, inc in zip(layer.params(),\n",
    "                                              layer.param_incs()):\n",
    "                            print(\"Layer has type:\",\n",
    "                                  layer.__class__.__name__)\n",
    "                            print(\"Layer has parameters shape:\",\n",
    "                                  param.shape)\n",
    "                            param -= learning_rate*inc\n",
    "                end = time.time()\n",
    "                print(\"Running batch\", b, \"through the network took\", round(end-batch_time, 3), \"seconds\")\n",
    "            # Output training status\n",
    "            loss = self._loss(X, Y_one_hot)\n",
    "            error = self.error(X, Y)\n",
    "            print('iter %i, loss %.4f, train error %.4f' % (iter, loss, error))\n",
    "\n",
    "    def _loss(self, X, Y_one_hot):\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        Y_pred = X_next\n",
    "        return self.layers[-1].loss(Y_one_hot, Y_pred)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        Y_pred = unhot(X_next)\n",
    "        return Y_pred\n",
    "\n",
    "    def error(self, X, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        Y_pred = self.predict(X)\n",
    "        error = Y_pred != Y\n",
    "        return np.mean(error)\n",
    "\n",
    "    def check_gradients(self, X, Y):\n",
    "        \"\"\" Helper function to test the parameter gradients for\n",
    "        correctness. \"\"\"\n",
    "        # Warning: the following is a hack\n",
    "        Y_one_hot = one_hot(Y)\n",
    "        self._setup(X, Y_one_hot)\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, ParamMixin):\n",
    "                print('layer %d' % l)\n",
    "                for p, param in enumerate(layer.params()):\n",
    "                    param_shape = param.shape\n",
    "\n",
    "                    def fun(param_new):\n",
    "                        param[:] = np.reshape(param_new, param_shape)\n",
    "                        return self._loss(X, Y_one_hot)\n",
    "\n",
    "                    def grad_fun(param_new):\n",
    "                        param[:] = np.reshape(param_new, param_shape)\n",
    "                        # Forward propagation\n",
    "                        X_next = X\n",
    "                        for layer in self.layers:\n",
    "                            X_next = layer.fprop(X_next)\n",
    "                        Y_pred = X_next\n",
    "\n",
    "                        # Back-propagation of partial derivatives\n",
    "                        next_grad = self.layers[-1].input_grad(Y_one_hot,\n",
    "                                                               Y_pred)\n",
    "                        for layer in reversed(self.layers[l:-1]):\n",
    "                            next_grad = layer.bprop(next_grad)\n",
    "                        return np.ravel(self.layers[l].param_grads()[p])\n",
    "\n",
    "                    param_init = np.ravel(np.copy(param))\n",
    "                    err = sp.optimize.check_grad(fun, grad_fun, param_init)\n",
    "                    print('diff %.2e' % err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(Layer, ParamMixin):\n",
    "    def __init__(self, n_feats, filter_shape, strides, weight_scale,\n",
    "                 weight_decay=0.0, padding_mode='same', border_mode='nearest'):\n",
    "        self.n_feats = n_feats\n",
    "        self.filter_shape = filter_shape\n",
    "        self.strides = strides\n",
    "        self.weight_scale = weight_scale\n",
    "        self.weight_decay = weight_decay\n",
    "        self.padding_mode = padding_mode\n",
    "        self.border_mode = border_mode\n",
    "\n",
    "    def _setup(self, input_shape, rng):\n",
    "        n_channels = input_shape[1]\n",
    "        W_shape = (n_channels, self.n_feats) + self.filter_shape\n",
    "        self.W = rng.normal(size=W_shape, scale=self.weight_scale)\n",
    "        self.b = np.zeros(self.n_feats)\n",
    "\n",
    "    def fprop(self, input):\n",
    "        self.last_input = input\n",
    "        self.last_input_shape = input.shape\n",
    "        convout = np.empty(self.output_shape(input.shape))\n",
    "        convout = conv_bc01(input, self.W, convout)\n",
    "        return convout + self.b[np.newaxis, :, np.newaxis, np.newaxis]\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        input_grad = np.empty(self.last_input_shape)\n",
    "        self.dW = np.empty(self.W.shape)\n",
    "        input_grad, self.dW = bprop_conv_bc01(self.last_input, output_grad,\n",
    "                                              self.W, input_grad, self.dW)\n",
    "        n_imgs = output_grad.shape[0]\n",
    "        self.db = np.sum(output_grad, axis=(0, 2, 3)) / (n_imgs)\n",
    "        self.dW -= self.weight_decay*self.W\n",
    "        return input_grad\n",
    "\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    def param_incs(self):\n",
    "        return self.dW, self.db\n",
    "\n",
    "    def param_grads(self):\n",
    "        # undo weight decay\n",
    "        gW = self.dW+self.weight_decay*self.W\n",
    "        return gW, self.db\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        if self.padding_mode == 'same':\n",
    "            h = input_shape[2]\n",
    "            w = input_shape[3]\n",
    "        elif self.padding_mode == 'full':\n",
    "            h = input_shape[2]-self.filter_shape[1]+1\n",
    "            w = input_shape[3]-self.filter_shape[2]+1\n",
    "        else:\n",
    "            h = input_shape[2]+self.filter_shape[1]-1\n",
    "            w = input_shape[3]+self.filter_shape[2]-1\n",
    "        shape = (input_shape[0], self.n_feats, h, w)\n",
    "        return shape\n",
    "\n",
    "\n",
    "class Pool(Layer):\n",
    "    def __init__(self, pool_shape=(3, 3), strides=(1, 1), mode='max'):\n",
    "        self.mode = mode\n",
    "        self.pool_h, self.pool_w = pool_shape\n",
    "        self.stride_y, self.stride_x = strides\n",
    "\n",
    "    def fprop(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        self.last_switches = np.empty(self.output_shape(input.shape)+(2,),\n",
    "                                      dtype=np.int)\n",
    "        poolout = np.empty(self.output_shape(input.shape))\n",
    "        poolout, switches = pool_bc01(input, poolout, self.last_switches, self.pool_h, self.pool_w,\n",
    "                  self.stride_y, self.stride_x)\n",
    "        self.last_switches = switches\n",
    "        return poolout\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        input_grad = np.empty(self.last_input_shape)\n",
    "        input_grad = bprop_pool_bc01(output_grad, self.last_switches, input_grad)\n",
    "        return input_grad\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        shape = (input_shape[0],\n",
    "                 input_shape[1],\n",
    "                 input_shape[2]//self.stride_y,\n",
    "                 input_shape[3]//self.stride_x)\n",
    "        return shape\n",
    "\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def fprop(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        return np.reshape(input, (input.shape[0], -1))\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        return np.reshape(output_grad, self.last_input_shape)\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        return (input_shape[0], np.prod(input_shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bc01(imgs, filters, convout):\n",
    "    \"\"\" Multi-image, multi-channel convolution\n",
    "    imgs has shape (n_imgs, n_channels_in, img_h, img_w)\n",
    "    For the base neural net, this is: (32, 1, 28, 28)\n",
    "    filters has shape (n_channels_in, n_channels_out, img_h, img_w)\n",
    "    For the base neural net, this is: (1, 12, 5, 5)\n",
    "    convout has shape (n_imgs, n_channels_out, img_h, img_w)\n",
    "    For the base neural net, this is: (32, 12, 28, 28)\n",
    "    \"\"\"\n",
    "    # TODO: support padding and striding  \n",
    "    # TODO: experiment with border mode 'reflect'  \n",
    "\n",
    "    n_imgs = imgs.shape[0]\n",
    "    img_h = imgs.shape[2]\n",
    "    img_w = imgs.shape[3]\n",
    "    n_channels_in = filters.shape[0]\n",
    "    n_channels_out = filters.shape[1]\n",
    "    fil_h = filters.shape[2]\n",
    "    fil_w = filters.shape[3]\n",
    "    \n",
    "    fil_mid_h = fil_h // 2 # 3\n",
    "    fil_mid_w = fil_w // 2 # 3\n",
    "\n",
    "    for i in range(n_imgs): # batch size\n",
    "        for c_out in range(n_channels_out): # number of filters (e.g. 12)\n",
    "            for y in range(img_h): \n",
    "                y_off_min = max(-y, -fil_mid_h) # Get the minimum value of the filter\n",
    "                # If y = 1, fil_mid_h = 3, so max(-1, -3) = -1, so the \n",
    "                # filter will only go one unit in the y direction.\n",
    "                y_off_max = min(img_h-y, fil_mid_h+1)\n",
    "                # Similarly, if we are \"close to the top of the image\", \n",
    "                # cut off the filter height.\n",
    "                for x in range(img_w):\n",
    "                    x_off_min = max(-x, -fil_mid_w)\n",
    "                    # Similar logic for the left of the filter...\n",
    "                    x_off_max = min(img_w-x, fil_mid_w+1)\n",
    "                    # ...and the right of the filter.\n",
    "                    value = 0.0\n",
    "                    for y_off in range(y_off_min, y_off_max):\n",
    "                        for x_off in range(x_off_min, x_off_max):\n",
    "                            # Loop through the range of the filter, for:\n",
    "                            # A given image.\n",
    "                            # A given channel.\n",
    "                            # A given height and width value in the image.\n",
    "                            img_y = y + y_off\n",
    "                            img_x = x + x_off\n",
    "                            fil_y = fil_mid_w + y_off\n",
    "                            fil_x = fil_mid_h + x_off\n",
    "                            # Get the correct pixel value and the correct image value\n",
    "                            for c_in in range(n_channels_in): # For each channel into the image\n",
    "                                value += imgs[i, c_in, img_y, img_x] * filters[c_in, c_out, fil_y, fil_x]\n",
    "                                # Add the value in:\n",
    "                                # the first image, in the first pixel value, times the filter\n",
    "                    # For the value in the first pixel, first channel, if the convolution size for\n",
    "                    # this value is 3 x 3, then the value in cell (1,1) in the first output neuron is:\n",
    "                    # imgs[1, 1, 1, 1] * filters[1, 1, 1, 1] + \n",
    "                    # imgs[1, 1, 1, 2] * filters[1, 1, 1, 2] +\n",
    "                    # imgs[1, 1, 1, 3] * filters[1, 1, 1, 3] +\n",
    "                    # imgs[1, 1, 2, 1] * filters[1, 1, 2, 1] +\n",
    "                    # imgs[1, 1, 2, 2] * filters[1, 1, 2, 2] +\n",
    "                    # imgs[1, 1, 2, 3] * filters[1, 1, 2, 3] +\n",
    "                    # imgs[1, 1, 3, 1] * filters[1, 1, 3, 1] +\n",
    "                    # imgs[1, 1, 3, 2] * filters[1, 1, 3, 2] +\n",
    "                    # imgs[1, 1, 3, 3] * filters[1, 1, 3, 3]\n",
    "                    convout[i, c_out, y, x] = value    \n",
    "    \n",
    "    return convout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bprop_conv_bc01(imgs, convout_grad, filters, imgs_grad, filters_grad):\n",
    "    \"\"\" Back-propagate gradients of multi-image, multi-channel convolution\n",
    "    Inputs:\n",
    "    imgs shape: (n_imgs, n_channels_in, img_h, img_w)\n",
    "    filters has shape (n_channels_in, n_channels_out, img_h, img_w)\n",
    "    convout_grad has same shape as convout: (n_imgs, n_channels_out, img_h, img_w)\n",
    "\n",
    "    Returns:\n",
    "    imgs_grad has same shape as imgs: (n_imgs, n_channels_in, img_h, img_w)\n",
    "    filters_grad has same shape as filters: (n_channels_in, n_channels_out, img_h, img_w)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n_imgs = convout_grad.shape[0]\n",
    "    img_h = convout_grad.shape[2]\n",
    "    img_w = convout_grad.shape[3]\n",
    "    n_channels_convout = filters.shape[1]\n",
    "    n_channels_imgs = filters.shape[0]\n",
    "    fil_h = filters.shape[2]\n",
    "    fil_w = filters.shape[3]\n",
    "    fil_mid_h = fil_h // 2\n",
    "    fil_mid_w = fil_w // 2\n",
    "\n",
    "    imgs_grad = np.zeros((n_imgs, n_channels_imgs, img_h, img_w)) # Same shape as images\n",
    "    filters_grad = np.zeros((n_channels_imgs, n_channels_convout, fil_h, fil_w))  # Same shape as filters\n",
    "    for i in range(n_imgs):\n",
    "        for c_convout in range(n_channels_convout):\n",
    "            for y in range(img_h):\n",
    "                # Get the minimum and maximum indices of the convolutional filters.\n",
    "                y_off_min = max(-y, -fil_mid_h)\n",
    "                y_off_max = min(img_h-y, fil_mid_h+1)\n",
    "                for x in range(img_w):           \n",
    "                    convout_grad_value = convout_grad[i, c_convout, y, x]\n",
    "                    # Get the minimum and maximum indices of the convolutional filters.\n",
    "                    # indices: (image, channel_out, image \"y\", image \"x\")\n",
    "                    x_off_min = max(-x, -fil_mid_w)\n",
    "                    x_off_max = min(img_w-x, fil_mid_w+1)\n",
    "                    for y_off in range(y_off_min, y_off_max):\n",
    "                        for x_off in range(x_off_min, x_off_max):\n",
    "                            img_y = y + y_off\n",
    "                            img_x = x + x_off\n",
    "                            fil_y = fil_mid_w + y_off\n",
    "                            fil_x = fil_mid_h + x_off\n",
    "                            # n_channels_imgs = channels in (e.g. 3)\n",
    "                            for c_imgs in range(n_channels_imgs):\n",
    "                                # for each \"image\" channel:\n",
    "                                imgs_grad[i, c_imgs, img_y, img_x] += filters[c_imgs, c_convout, fil_y, fil_x] * convout_grad_value\n",
    "                                # Add to that value in the image gradient:\n",
    "                                # the sum of all the values from the filters from that particular image channel, times the gradient\n",
    "                                # for this convout layer\n",
    "                                filters_grad[c_imgs, c_convout, fil_y, fil_x] += imgs[i, c_imgs, img_y, img_x] * convout_grad_value\n",
    "                                # As for the filters gradient: add the values in the sum of all the images:\n",
    "                                # Add the sum of all the values *across all images* (as with regular gradient descent):\n",
    "                                # Of: all the values that were multiplied by that weight in the forward pass, \n",
    "                                # times the convout_grad_value\n",
    "    # Divide the filter_grad by the number of images\n",
    "    filters_grad /= n_imgs\n",
    "\n",
    "    \n",
    "    return imgs_grad, filters_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_bc01(imgs, poolout, switches, pool_h,\n",
    "              pool_w, stride_y, stride_x):\n",
    "    \"\"\" Multi-image, multi-channel pooling\n",
    "    imgs has shape (n_imgs, n_channels, img_h, img_w)\n",
    "    poolout has shape (n_imgs, n_channels, img_h//stride_y, img_w//stride_x)\n",
    "    switches has shape (n_imgs, n_channels, img_h//stride_y, img_w//stride_x, 2)\n",
    "    \"\"\"\n",
    "    # TODO: mean pool\n",
    "\n",
    "    n_imgs = imgs.shape[0]\n",
    "    n_channels = imgs.shape[1]\n",
    "    img_h = imgs.shape[2]\n",
    "    img_w = imgs.shape[3]\n",
    "\n",
    "    out_h = img_h // stride_y # 14\n",
    "    out_w = img_w // stride_x # 14\n",
    "\n",
    "    pool_h_top = pool_h // 2 - 1 + pool_h % 2 # 0 \n",
    "    pool_h_bottom = pool_h // 2 + 1 # 2\n",
    "    pool_w_left = pool_w // 2 - 1 + pool_w % 2 # 0\n",
    "    pool_w_right = pool_w // 2 + 1 # 2\n",
    "\n",
    "    if not n_imgs == poolout.shape[0] == switches.shape[0]:\n",
    "        raise ValueError('Mismatch in number of images.')\n",
    "    if not n_channels == poolout.shape[1] == switches.shape[1]:\n",
    "        raise ValueError('Mismatch in number of channels.')\n",
    "    if not (out_h == poolout.shape[2] == switches.shape[2] and out_w == poolout.shape[3] == switches.shape[3]):\n",
    "        raise ValueError('Mismatch in image shape.')\n",
    "    if not switches.shape[4] == 2:\n",
    "        raise ValueError('switches should only have length 2 in the 5. dimension.')\n",
    "\n",
    "    img_y_max = 0\n",
    "    img_x_max = 0\n",
    "\n",
    "    poolout = np.zeros((n_imgs, n_channels, out_h, out_w))\n",
    "    for i in range(n_imgs):\n",
    "        for c in range(n_channels):\n",
    "            for y_out in range(out_h):\n",
    "                y = y_out*stride_y # move along by stride_y\n",
    "                # min will either 0 or (usually) the bottom of the image area\n",
    "                y_min = max(y-pool_h_top, 0)\n",
    "                # max will either 28 or (usually) the top of the image area\n",
    "                y_max = min(y+pool_h_bottom, img_h)\n",
    "                # Calculate the same for x\n",
    "                for x_out in range(out_w):\n",
    "                    # move along by stride_x\n",
    "                    x = x_out*stride_x\n",
    "                    x_min = max(x-pool_w_left, 0)\n",
    "                    x_max = min(x+pool_w_right, img_w)\n",
    "                    value = -9e99\n",
    "                    for img_y in range(y_min, y_max):\n",
    "                        for img_x in range(x_min, x_max):\n",
    "                            # Within the correct image area: double for loop to find the max\n",
    "                            new_value = imgs[i, c, img_y, img_x]\n",
    "                            if new_value > value:\n",
    "                                value = new_value\n",
    "                                img_y_max = img_y\n",
    "                                img_x_max = img_x\n",
    "                    # Define poolout for this image and channel to be the max value\n",
    "                    poolout[i, c, y_out, x_out] = value\n",
    "                    # Define the switches to be the values that indices that contained the maxima\n",
    "                    # in each pool.\n",
    "                    switches[i, c, y_out, x_out, 0] = img_y_max\n",
    "                    switches[i, c, y_out, x_out, 1] = img_x_max\n",
    "                    \n",
    "    return poolout, switches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bprop_pool_bc01(poolout_grad, switches, imgs_grad):\n",
    "    \"\"\" Multi-image, multi-channel pooling\n",
    "    imgs_grad has same shape as imgs (n_imgs, n_channels, img_h, img_w)\n",
    "    poolout_grad has same shape as poolout: (n_imgs, n_channels, img_h//stride_y, img_w//stride_x)\n",
    "    switches has shape (n_imgs, n_channels, img_h//stride_y, img_w//stride_x, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    # \"poolout\" = \"*Output* of pooling\n",
    "    n_imgs = poolout_grad.shape[0] # 32\n",
    "    n_channels = poolout_grad.shape[1] # 12 *number of channels of the layer \"prior\" to pooling*\n",
    "    poolout_h = poolout_grad.shape[2] # 14\n",
    "    poolout_w = poolout_grad.shape[3] # 14\n",
    "\n",
    "#     imgs_grad = np.zeros((n_imgs, n_channels, imgs_grad.shape[2], imgs_grad.shape[3]))\n",
    "    # For each image\n",
    "    for i in range(n_imgs):\n",
    "        # For each channel \n",
    "        for c in range(n_channels):\n",
    "            for y in range(poolout_h):\n",
    "                for x in range(poolout_w):\n",
    "                    # Double for loop over 14 x 14\n",
    "                    # Get the indices in the prior image that contained the max values.\n",
    "                    img_y = switches[i, c, y, x, 0]\n",
    "                    img_x = switches[i, c, y, x, 1]\n",
    "                    # Make the gradient of those pixels in the images equal to\n",
    "                    # the poolout gradient (the output gradient of the pooling \n",
    "                    # layer at those pixel values.\n",
    "                    imgs_grad[i, c, img_y, img_x] = poolout_grad[i, c, y, x]\n",
    "    return imgs_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(mnist_data[:split], (-1, 1, 28, 28))/255.0\n",
    "y_train = mnist_target[:split]\n",
    "X_test = np.reshape(mnist_data[split:], (-1, 1, 28, 28))/255.0\n",
    "y_test = mnist_target[split:]\n",
    "n_classes = np.unique(y_train).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample training data\n",
    "n_train_samples = 3000\n",
    "train_idxs = np.random.randint(0, split-1, n_train_samples)\n",
    "X_train = X_train[train_idxs, ...]\n",
    "y_train = y_train[train_idxs, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup convolutional neural network\n",
    "nn = NeuralNetwork(\n",
    "    layers=[\n",
    "        Conv(\n",
    "            n_feats=12,\n",
    "            filter_shape=(5, 5),\n",
    "            strides=(1, 1),\n",
    "            weight_scale=0.1,\n",
    "            weight_decay=0.001,\n",
    "        ),\n",
    "        Activation('relu'),\n",
    "        Pool(\n",
    "            pool_shape=(2, 2),\n",
    "            strides=(2, 2),\n",
    "            mode='max',\n",
    "        ),\n",
    "        Conv(\n",
    "            n_feats=16,\n",
    "            filter_shape=(5, 5),\n",
    "            strides=(1, 1),\n",
    "            weight_scale=0.1,\n",
    "            weight_decay=0.001,\n",
    "        ),\n",
    "        Activation('relu'),\n",
    "        Flatten(),\n",
    "        Linear(\n",
    "            n_out=n_classes,\n",
    "            weight_scale=0.1,\n",
    "            weight_decay=0.02,\n",
    "        ),\n",
    "        LogRegression(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Train neural network\n",
    "t0 = time.time()\n",
    "nn.fit(X_train, y_train, learning_rate=0.05, max_iter=3, batch_size=32)\n",
    "t1 = time.time()\n",
    "print('Duration: %.1fs' % (t1-t0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "231px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
