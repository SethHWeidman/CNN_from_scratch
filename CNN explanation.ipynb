{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A look inside convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a look inside convolutional neural networks. We'll be building them from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_net import conv_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello convolution!\n"
     ]
    }
   ],
   "source": [
    "conv_helpers.hello_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy as sp\n",
    "\n",
    "import time\n",
    "\n",
    "def read_idx(filename, path='./mldata/'):\n",
    "    with open(path + filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n",
    "\n",
    "    # Create an iterator which returns each image in turn\n",
    "    # for i in xrange(len(lbl)):\n",
    "    #     yield get_img(i)\n",
    "\n",
    "    return img, lbl\n",
    "\n",
    "def show(image):\n",
    "    \"\"\"\n",
    "    Render a given numpy.uint8 2D array of pixel data.\n",
    "    \"\"\"\n",
    "    from matplotlib import pyplot\n",
    "    import matplotlib as mpl\n",
    "    fig = pyplot.figure()\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image, cmap=mpl.cm.Greys)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.xaxis.set_ticks_position('top')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "def one_hot(labels):\n",
    "    classes = np.unique(labels).astype(int)\n",
    "    n_classes = classes.size\n",
    "    one_hot_labels = np.zeros(labels.shape + (n_classes,))\n",
    "    for c in classes:\n",
    "        one_hot_labels[labels == c, c] = 1\n",
    "    return one_hot_labels\n",
    "\n",
    "\n",
    "def unhot(one_hot_labels):\n",
    "    return np.argmax(one_hot_labels, axis=-1)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_d(x):\n",
    "    s = sigmoid(x)\n",
    "    return s*(1-s)\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_d(x):\n",
    "    e = np.exp(2*x)\n",
    "    return (e-1)/(e+1)\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "\n",
    "def relu_d(x):\n",
    "    dx = np.zeros(x.shape)\n",
    "    dx[x >= 0] = 1\n",
    "    return dx\n",
    "\n",
    "class Layer(object):\n",
    "    def _setup(self, input_shape, rng):\n",
    "        \"\"\" Setup layer with parameters that are unknown at __init__(). \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fprop(self, input):\n",
    "        \"\"\" Calculate layer output for given input (forward propagation). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        \"\"\" Calculate input gradient. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        \"\"\" Calculate shape of this layer's output.\n",
    "        input_shape[0] is the number of samples in the input.\n",
    "        input_shape[1:] is the shape of the feature.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class LossMixin(object):\n",
    "    def loss(self, output, output_pred):\n",
    "        \"\"\" Calculate mean loss given output and predicted output. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def input_grad(self, output, output_pred):\n",
    "        \"\"\" Calculate input gradient given output and predicted output. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class ParamMixin(object):\n",
    "    def params(self):\n",
    "        \"\"\" Layer parameters. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def param_grads(self):\n",
    "        \"\"\" Get layer parameter gradients as calculated from bprop(). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def param_incs(self):\n",
    "        \"\"\" Get layer parameter steps as calculated from bprop(). \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class Linear(Layer, ParamMixin):\n",
    "    def __init__(self, n_out, weight_scale, weight_decay=0.0):\n",
    "        self.n_out = n_out\n",
    "        self.weight_scale = weight_scale\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def _setup(self, input_shape, rng):\n",
    "        n_input = input_shape[1]\n",
    "        W_shape = (n_input, self.n_out)\n",
    "        self.W = rng.normal(size=W_shape, scale=self.weight_scale)\n",
    "        self.b = np.zeros(self.n_out)\n",
    "\n",
    "    def fprop(self, input):\n",
    "        self.last_input = input\n",
    "        return np.dot(input, self.W) + self.b\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        n = output_grad.shape[0]\n",
    "        self.dW = np.dot(self.last_input.T, output_grad)/n - self.weight_decay*self.W\n",
    "        self.db = np.mean(output_grad, axis=0)\n",
    "        return np.dot(output_grad, self.W.T)\n",
    "\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    def param_incs(self):\n",
    "        return self.dW, self.db\n",
    "\n",
    "    def param_grads(self):\n",
    "        # undo weight decay to get gradient\n",
    "        gW = self.dW+self.weight_decay*self.W\n",
    "        return gW, self.db\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.n_out)\n",
    "\n",
    "\n",
    "class Activation(Layer):\n",
    "    def __init__(self, type):\n",
    "        if type == 'sigmoid':\n",
    "            self.fun = sigmoid\n",
    "            self.fun_d = sigmoid_d\n",
    "        elif type == 'relu':\n",
    "            self.fun = relu\n",
    "            self.fun_d = relu_d\n",
    "        elif type == 'tanh':\n",
    "            self.fun = tanh\n",
    "            self.fun_d = tanh_d\n",
    "        else:\n",
    "            raise ValueError('Invalid activation function.')\n",
    "\n",
    "    def fprop(self, input):\n",
    "        self.last_input = input\n",
    "        return self.fun(input)\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        return output_grad*self.fun_d(self.last_input)\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "class LogRegression(Layer, LossMixin):\n",
    "    \"\"\" Softmax layer with cross-entropy loss function. \"\"\"\n",
    "    def fprop(self, input):\n",
    "        e = np.exp(input - np.amax(input, axis=1, keepdims=True))\n",
    "        return e/np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        raise NotImplementedError(\n",
    "            'LogRegression does not support back-propagation of gradients. '\n",
    "            + 'It should occur only as the last layer of a NeuralNetwork.'\n",
    "        )\n",
    "\n",
    "    def input_grad(self, Y, Y_pred):\n",
    "        # Assumes one-hot encoding.\n",
    "        return -(Y - Y_pred)\n",
    "\n",
    "    def loss(self, Y, Y_pred):\n",
    "        # Assumes one-hot encoding.\n",
    "        eps = 1e-15\n",
    "        Y_pred = np.clip(Y_pred, eps, 1 - eps)\n",
    "        Y_pred /= Y_pred.sum(axis=1, keepdims=True)\n",
    "        loss = -np.sum(Y * np.log(Y_pred))\n",
    "        return loss / Y.shape[0]\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, rng=None):\n",
    "        self.layers = layers\n",
    "        if rng is None:\n",
    "            rng = np.random.RandomState()\n",
    "        self.rng = rng\n",
    "\n",
    "    def _setup(self, X, Y):\n",
    "        # Setup layers sequentially\n",
    "        next_shape = X.shape\n",
    "        for layer in self.layers:\n",
    "            layer._setup(next_shape, self.rng)\n",
    "            next_shape = layer.output_shape(next_shape)\n",
    "#            print(next_shape)\n",
    "        if next_shape != Y.shape:\n",
    "            raise ValueError('Output shape %s does not match Y %s'\n",
    "                             % (next_shape, Y.shape))\n",
    "\n",
    "    def fit(self, X, Y, learning_rate=0.1, max_iter=10, batch_size=64):\n",
    "        \"\"\" Train network on the given data. \"\"\"\n",
    "        n_samples = Y.shape[0]\n",
    "        n_batches = n_samples // batch_size\n",
    "        Y_one_hot = one_hot(Y)\n",
    "        self._setup(X, Y_one_hot)\n",
    "        iter = 0\n",
    "        # Stochastic gradient descent with mini-batches\n",
    "        while iter < max_iter:\n",
    "            iter += 1\n",
    "            for b in range(n_batches):\n",
    "                batch_start = time.time()\n",
    "                batch_begin = b*batch_size\n",
    "                batch_end = batch_begin+batch_size\n",
    "                X_batch = X[batch_begin:batch_end]\n",
    "                Y_batch = Y_one_hot[batch_begin:batch_end]\n",
    "\n",
    "                # Forward propagation\n",
    "                X_next = X_batch\n",
    "                for layer in self.layers:\n",
    "                    X_next = layer.fprop(X_next)\n",
    "                Y_pred = X_next\n",
    "\n",
    "                # Back propagation of partial derivatives\n",
    "                next_grad = self.layers[-1].input_grad(Y_batch, Y_pred)\n",
    "                for layer in reversed(self.layers[:-1]):\n",
    "                    next_grad = layer.bprop(next_grad)\n",
    "\n",
    "                # Update parameters\n",
    "                for layer in self.layers:\n",
    "                    if isinstance(layer, ParamMixin):\n",
    "                        for param, inc in zip(layer.params(),\n",
    "                                              layer.param_incs()):\n",
    "                            param -= learning_rate*inc\n",
    "                batch_end = time.time()\n",
    "                print(\"Running batch\", b, \"through the network took\", round(batch_end-batch_start, 3), \"seconds\")\n",
    "            # Output training status\n",
    "            print(\"Computing loss\")\n",
    "            loss_start = time.time()\n",
    "            loss = self._loss(X, Y_one_hot)\n",
    "            print(\"Loss\", loss)\n",
    "            loss_end = time.time()\n",
    "            print(\"Computing loss\", loss, \"took\", round(loss_end-loss_start, 3), \"seconds\")\n",
    "            print(\"Computing error\")\n",
    "            error_start = time.time()\n",
    "            error = self.error(X, Y)\n",
    "            error_end = time.time()\n",
    "            print(\"Error\", error)\n",
    "            print(\"Computing error\", error, \"took\", round(error_end-error_start, 3), \"seconds\")\n",
    "            print('iter %i, loss %.4f, train error %.4f' % (iter, loss, error))\n",
    "\n",
    "    def _loss(self, X, Y_one_hot):\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        Y_pred = X_next\n",
    "        return self.layers[-1].loss(Y_one_hot, Y_pred)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        X_next = X\n",
    "        for layer in self.layers:\n",
    "            X_next = layer.fprop(X_next)\n",
    "        Y_pred = unhot(X_next)\n",
    "        return Y_pred\n",
    "\n",
    "    def error(self, X, Y):\n",
    "        \"\"\" Calculate error on the given data. \"\"\"\n",
    "        Y_pred = self.predict(X)\n",
    "        error = Y_pred != Y\n",
    "        return np.mean(error)\n",
    "\n",
    "    def check_gradients(self, X, Y):\n",
    "        \"\"\" Helper function to test the parameter gradients for\n",
    "        correctness. \"\"\"\n",
    "        # Warning: the following is a hack\n",
    "        Y_one_hot = one_hot(Y)\n",
    "        self._setup(X, Y_one_hot)\n",
    "        for l, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, ParamMixin):\n",
    "                print('layer %d' % l)\n",
    "                for p, param in enumerate(layer.params()):\n",
    "                    param_shape = param.shape\n",
    "\n",
    "                    def fun(param_new):\n",
    "                        param[:] = np.reshape(param_new, param_shape)\n",
    "                        return self._loss(X, Y_one_hot)\n",
    "\n",
    "                    def grad_fun(param_new):\n",
    "                        param[:] = np.reshape(param_new, param_shape)\n",
    "                        # Forward propagation\n",
    "                        X_next = X\n",
    "                        for layer in self.layers:\n",
    "                            X_next = layer.fprop(X_next)\n",
    "                        Y_pred = X_next\n",
    "\n",
    "                        # Back-propagation of partial derivatives\n",
    "                        next_grad = self.layers[-1].input_grad(Y_one_hot,\n",
    "                                                               Y_pred)\n",
    "                        for layer in reversed(self.layers[l:-1]):\n",
    "                            next_grad = layer.bprop(next_grad)\n",
    "                        return np.ravel(self.layers[l].param_grads()[p])\n",
    "\n",
    "                    param_init = np.ravel(np.copy(param))\n",
    "                    err = sp.optimize.check_grad(fun, grad_fun, param_init)\n",
    "                    print('diff %.2e' % err)\n",
    "\n",
    "\n",
    "class Conv(Layer, ParamMixin):\n",
    "    def __init__(self, n_feats, filter_shape, strides, weight_scale,\n",
    "                 weight_decay=0.0, padding_mode='same', border_mode='nearest'):\n",
    "        self.n_feats = n_feats\n",
    "        self.filter_shape = filter_shape\n",
    "        self.strides = strides\n",
    "        self.weight_scale = weight_scale\n",
    "        self.weight_decay = weight_decay\n",
    "        self.padding_mode = padding_mode\n",
    "        self.border_mode = border_mode\n",
    "\n",
    "    def _setup(self, input_shape, rng):\n",
    "        n_channels = input_shape[1]\n",
    "        W_shape = (n_channels, self.n_feats) + self.filter_shape\n",
    "        self.W = rng.normal(size=W_shape, scale=self.weight_scale)\n",
    "        self.b = np.zeros(self.n_feats)\n",
    "\n",
    "    def fprop(self, input):\n",
    "        self.last_input = input\n",
    "        self.last_input_shape = input.shape\n",
    "        convout = np.empty(self.output_shape(input.shape))\n",
    "        convout = conv_bc01(input, self.W, convout)\n",
    "        return convout + self.b[np.newaxis, :, np.newaxis, np.newaxis]\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        input_grad = np.empty(self.last_input_shape)\n",
    "        self.dW = np.empty(self.W.shape)\n",
    "        input_grad, self.dW = bprop_conv_bc01(self.last_input, output_grad,\n",
    "                                              self.W, input_grad, self.dW)\n",
    "        n_imgs = output_grad.shape[0]\n",
    "        self.db = np.sum(output_grad, axis=(0, 2, 3)) / (n_imgs)\n",
    "        self.dW -= self.weight_decay*self.W\n",
    "        return input_grad\n",
    "\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    def param_incs(self):\n",
    "        return self.dW, self.db\n",
    "\n",
    "    def param_grads(self):\n",
    "        # undo weight decay\n",
    "        gW = self.dW+self.weight_decay*self.W\n",
    "        return gW, self.db\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        if self.padding_mode == 'same':\n",
    "            h = input_shape[2]\n",
    "            w = input_shape[3]\n",
    "        elif self.padding_mode == 'full':\n",
    "            h = input_shape[2]-self.filter_shape[1]+1\n",
    "            w = input_shape[3]-self.filter_shape[2]+1\n",
    "        else:\n",
    "            h = input_shape[2]+self.filter_shape[1]-1\n",
    "            w = input_shape[3]+self.filter_shape[2]-1\n",
    "        shape = (input_shape[0], self.n_feats, h, w)\n",
    "        return shape\n",
    "\n",
    "\n",
    "class Pool(Layer):\n",
    "    def __init__(self, pool_shape=(3, 3), strides=(1, 1), mode='max'):\n",
    "        self.mode = mode\n",
    "        self.pool_h, self.pool_w = pool_shape\n",
    "        self.stride_y, self.stride_x = strides\n",
    "\n",
    "    def fprop(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        self.last_switches = np.empty(self.output_shape(input.shape)+(2,),\n",
    "                                      dtype=np.int)\n",
    "        poolout = np.empty(self.output_shape(input.shape))\n",
    "        poolout, switches = pool_bc01(input, poolout, self.last_switches, self.pool_h, self.pool_w,\n",
    "                  self.stride_y, self.stride_x)\n",
    "        self.last_switches = switches\n",
    "        return poolout\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        input_grad = np.empty(self.last_input_shape)\n",
    "        input_grad = bprop_pool_bc01(output_grad, self.last_switches, input_grad)\n",
    "        return input_grad\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        shape = (input_shape[0],\n",
    "                 input_shape[1],\n",
    "                 input_shape[2]//self.stride_y,\n",
    "                 input_shape[3]//self.stride_x)\n",
    "        return shape\n",
    "\n",
    "\n",
    "class Flatten(Layer):\n",
    "    def fprop(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        return np.reshape(input, (input.shape[0], -1))\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        return np.reshape(output_grad, self.last_input_shape)\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        return (input_shape[0], np.prod(input_shape[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How are layers in a convolutional neural network connected?\n",
    "\n",
    "If we start off with an image that is 32 x 32 x 3 - color images - how can we change this to a convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We've all see diagrams like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How does the convolution actually happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's consider an individual 3 x 3 filter that is slid over an image. How will this filter be connected to the next layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The top left corner of the filter will be multiplied by the appropriate range of pixels in the output image (perhaps excluding the bottom right). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In addition, if the input has three channels, the filter will be multiplied by each channel of each part of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The gradient that the filters will receive is of shape:\n",
    "\n",
    "[1, 12, 28, 28]\n",
    "\n",
    "Essentially, 12 channels.\n",
    "\n",
    "The images being multiplied by these filters are of shape:\n",
    "\n",
    "[1, 1 (of 3), 28, 28]\n",
    "\n",
    "For the filters, the element:\n",
    "\n",
    "[1 (of 3), 1 (of 12), 1, 1]\n",
    "\n",
    "contributes to certain outputs in the next layer that are all the places where a pixel in the image was multiplied by a this particular filter value to get a particular value in the output. For image size of 28x28 and stride of 5, this would correspond to roughly a 24 x 24 patch. \n",
    "\n",
    "So, _the sum takes place for a given input channel and output channel_, across all the image locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For images, let's consider a pixel in input channel 1, and where affects the next layer. It is going to affect all 12 of the filters - and again, affect them based on the components of the filter that it has been multiplied by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "An insight: even though the channels differ in the convout, a given filter location and image location maps to one particular convout location. \n",
    "\n",
    "The converse is not true: a given convout location is mapped to by several filter-image location combinations. \n",
    "\n",
    "So, the overall strategy will be:\n",
    "1. Fix a convout location (including the channel_out)\n",
    "2. Find all the filter x and y locations that map to that (will usually be all, unless we're on an edge)\n",
    "3. Find the image locations that correspond to that convout location (these should correspond to the filter locations)\n",
    "4. Update the _filter gradient_ by looping over the _image locations_ that map to this convout location.\n",
    "5. Update the _image gradient_ by looping over the _filter locations_ that map to this convout location. \n",
    "6. Do steps 4 and 5 for each input image channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Next steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So, getting the filter gradient, and the image gradient, and the convout gradient to line up correctly involves looping over:\n",
    "1. Fix a location in the convout:\n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
