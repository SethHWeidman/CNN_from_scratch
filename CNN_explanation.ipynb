{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add explanations:\n",
    "\n",
    "1. Explanation of how regular neural networks are connected\n",
    "    a. Link to presentation\n",
    "2. Explanation of conv net concepts:\n",
    "    a. Pooling\n",
    "    b. Stride\n",
    "\n",
    "Add visuals:\n",
    "1. Connection of neuron to other neuron\n",
    "2. Convolutional neural \n",
    "\n",
    "Good enough so that it could be presented at a Meetup.\n",
    "\n",
    "Goal: draft done in next two days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive into Convolutional Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is intended for people who want to deep dive into how convolutional neural nets work. We've all seen diagrams like the following in the context of convolutional neural nets:\n",
    "\n",
    "![alt text](img/conv_net_architecture.png)\n",
    "\n",
    "This is great as a high level description of what is going on, but doesn't give us a detailed look at what is going on and why it works.\n",
    "\n",
    "I'm going to assume the reader is somewhat familiar with convolutional neural net concepts; I'll cover these concepts, but quickly, referring to you to other excellent and extensive resources if you want to learn more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of ConvNets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets suppose you just learned about neural networks and how fully connected layers work. Convolutional Neural Networks are a kind of neural network structure that is specialized for images. In regular neural nets, we pass in an input that has many features and let the neural net adjust its weights to learn which combinations of features are helpful for predicting the output. Convolutional neural nets are so-named because they use a special type of feature detector that is specialized for detecting features in images: a \"convolutional filter\". These filters can detect the presence of vertical and horizontal lines in raw images, for example; once these have been detected, they can detect more complicated combinations of features that can represent real-world shapes such as eyes, wings, or legs. \n",
    "\n",
    "For more, [this phenomenal essay] is the standard starting point for learning the details about convolutional neural nets.\n",
    "\n",
    "### In the weeds\n",
    "\n",
    "Getting into more detail: we saw that regular neural nets pass information forward by making each layer a linear combination of all the neurons in the prior layer. Convolutional neural nets instead encapsulate their weights in the filters, and then share these weights across an entire image.\n",
    "\n",
    "For example, let's say we had a 28 x 28 image, and we were applying convolutions of size 5 x 5 to it. Let's also say we used padding, so that applying the convolution resulted in an image of the same size. Also, let's say we applied 10 filters. Then there would be 5 x 5 x 10 weights in the filters, and the output layer that results would be 28 x 28 x 10. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all well and good for understanding how convolutional neural nets might make good predictions once they are trained. To understand how they learn, we'll have to:\n",
    "\n",
    "1. Understand how neural nets propagate information between neurons that are connnected via weights, and then\n",
    "2. Understand how the neurons and weights are actually connected in a convolutional neural network.\n",
    "\n",
    "## How neural networks propagate information\n",
    "\n",
    "To review: neural nets can be thought of as complicated mathematical functions with many parametere. The goal of neural net architectures is to find a function $f$ that maps\n",
    "\n",
    "They are trained by the backpropagation algorithm, which iteratively:\n",
    "\n",
    "1. Makes predictions using f_theta\n",
    "2. Calculates loss\n",
    "3. Calculates the derivatives of the parameters with respect to the loss\n",
    "4. Updates the parameters accordingly.\n",
    "\n",
    "For a deeper dive, see the talk [here](need link). TODO\n",
    "\n",
    "\n",
    "But: how does this look from the perspective of an individual neuron? \n",
    "\n",
    "Let's say an individual neuron, call it $n_1$ is connected to three other neurons, call them $o_1$, $o_2$, and $o_3$. This means that each of these $o$ neurons is a linear combination of $n_1$ and some other neurons.\n",
    "\n",
    "In addition, these $o$ neurons have gradients, we'll call them $g$ - amounts by which they ultimately affect the loss $L$. \n",
    "\n",
    "The neuron $n_1$ affects the output by\n",
    "\n",
    "$$ g_1 * w_{11} + g_2 * w_{12} + g_3 * w_{13} $$\n",
    "\n",
    "This is the amount, therefore, that we'll need to change the value of this neuron by: _for all of the neurons it is connected to, the sum over all of those neurons' gradients, weighted by the value of the weights connecting those neurons._\n",
    "\n",
    "What about the weights: how much should these be updated by? Recall that in convolutional neural nets weights are shared across multiple neurons, so that we might have a situation like the following:\n",
    "\n",
    "(TODO: insert image)\n",
    "\n",
    "How should $w_{11}$ be updated in this case? Let's say that, as before, the output neurons have gradients $g_i$. These weights would be updated according to \n",
    "\n",
    "$$ n_1 * g_1 + n_2 * g_2 + n_3 * g_3 $$\n",
    "\n",
    "that is, the sum over all of the \"input\" neurons it is connected to, weighted by the gradients of the output neurons that those input neurons were connected to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How convolutional neural networks are connected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know how to build networks with fully connected layers from scratch. But how do we do the same for convolutional networks?\n",
    "\n",
    "There will be two components to this explanation:\n",
    "\n",
    "1. Understanding how the neurons are connected.\n",
    "2. Figuring out how to code up these connections.\n",
    "\n",
    "Consider an image with one channel - a black and white, 7x7 image. Suppose we want to perform a convolution on this image with 10 filters, ending up with 10 7x7 layers each of which has had some features extracted from it. How can we do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ I = \\begin{bmatrix}i_{11} & i_{12} & i_{13} & i_{14} & i_{15}\\\\\n",
    "                      i_{21} & i_{22} & i_{23} & i_{24} & i_{25}\\\\\n",
    "                      i_{31} & i_{32} & i_{33} & i_{34} & i_{35}\\\\\n",
    "                      i_{41} & i_{42} & i_{43} & i_{44} & i_{45}\\\\\n",
    "                      i_{51} & i_{52} & i_{53} & i_{54} & i_{55}\\\\\n",
    "                      \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F = \\begin{bmatrix}f_{11}^1 & f_{12}^1 & f_{13}^1 \\\\\n",
    "                      f_{21}^1 & f_{22}^1 & f_{23}^1 \\\\\n",
    "                      f_{31}^1 & f_{32}^1 & f_{33}^1 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ I = \\begin{bmatrix}0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "                      0 & i_{11} & i_{12} & i_{13} & i_{14} & i_{15} & 0\\\\\n",
    "                      0 & i_{21} & i_{22} & i_{23} & i_{24} & i_{25} & 0\\\\\n",
    "                      0 & i_{31} & i_{32} & i_{33} & i_{34} & i_{35} & 0\\\\\n",
    "                      0 & i_{41} & i_{42} & i_{43} & i_{44} & i_{45} & 0\\\\\n",
    "                      0 & i_{51} & i_{52} & i_{53} & i_{54} & i_{55} & 0\\\\\n",
    "                      0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "                      \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the top left-most element would simply be:\n",
    "\n",
    "$ 0 * f_{11}^1 + 0 * f_{12}^1 + 0 * f_{21}^1 + i_{11} * f_{22}^1 $\n",
    "\n",
    "The one to the right of that would be:\n",
    "\n",
    "$ 0 * f_{11}^1 + 0 * f_{12}^1 + i_{11} * f_{21}^1 + i_{12} * f_{22}^1 $\n",
    "\n",
    "etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be 4 output images $O$:\n",
    "\n",
    "$$ O_1 = \\begin{bmatrix}o_{11}^1 & o_{12}^1 & o_{13}^1 & o_{14}^1 & o_{15}^1\\\\\n",
    "                      o_{21}^1 & o_{22}^1 & o_{23}^1 & o_{24}^1 & o_{25}^1\\\\\n",
    "                      o_{31}^1 & o_{32}^1 & o_{33}^1 & o_{34}^1 & o_{35}^1\\\\\n",
    "                      o_{41}^1 & o_{42}^1 & o_{43}^1 & o_{44}^1 & o_{45}^1\\\\\n",
    "                      o_{51}^1 & o_{52}^1 & o_{53}^1 & o_{54}^1 & o_{55}^1\\\\\n",
    "                      \\end{bmatrix} $$\n",
    "                      \n",
    "$$ O_2 = \\begin{bmatrix}o_{11}^2 & o_{12}^2 & o_{13}^2 & o_{14}^2 & o_{15}^2\\\\\n",
    "                      o_{21}^2 & o_{22}^2 & o_{23}^2 & o_{24}^2 & o_{25}^2\\\\\n",
    "                      o_{31}^2 & o_{32}^2 & o_{33}^2 & o_{34}^2 & o_{35}^2\\\\\n",
    "                      o_{41}^2 & o_{42}^2 & o_{43}^2 & o_{44}^2 & o_{45}^2\\\\\n",
    "                      o_{51}^2 & o_{52}^2 & o_{53}^2 & o_{54}^2 & o_{55}^2\\\\\n",
    "                      \\end{bmatrix} $$\n",
    "                      \n",
    "and so on for $O_3$ and $O_4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward propogation step is where things get interesting. We need to figure out how much each pixel in the input image affects the following layer and how much each element in each filter affects the following layer.\n",
    "\n",
    "Consider a pixel in the input image $i_{22}$. This pixel will affect the first output image in 9 different ways: one for each place it has been multiplied by in the filters:\n",
    "\n",
    "$$o_{11}^1 = f_{33}^1 * i_{22} + ...$$\n",
    "$$o_{12}^1 = f_{32}^1 * i_{22} + ...$$\n",
    "$$o_{13}^1 = f_{31}^1 * i_{22} + ...$$\n",
    "$$o_{21}^1 = f_{23}^1 * i_{22} + ...$$\n",
    "$$o_{22}^1 = f_{22}^1 * i_{22} + ...$$\n",
    "$$o_{23}^1 = f_{21}^1 * i_{22} + ...$$\n",
    "$$o_{31}^1 = f_{11}^1 * i_{22} + ...$$\n",
    "$$o_{32}^1 = f_{12}^1 * i_{22} + ...$$\n",
    "$$o_{33}^1 = f_{13}^1 * i_{22} + ...$$\n",
    "\n",
    "Furthermore, it will \n",
    "\n",
    "So, to code up the appropriate weight update for the filters, we would need to loop over the output images, and inside this loop, loop over the appropriate filter-ouptut image pairs for that input pixel (the nine filter-output image pairs above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to code up the appropriate weight update for the filters, we would need to loop over the output images, and inside this loop, loop over the appropriate filter-ouptut image pairs for that input pixel (the nine filter-output image pairs above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the filters: each filter will be multiplied by almost all of the pixels in the input image and almost all of the pixels in the output image. For example, for the filter pixel $f_{11}^1$, it will be slid over all of the images in the input array, so you'll have:\n",
    "\n",
    "$$o_{22}^1 = i_{11} * f_{11}^1 + ...$$\n",
    "$$o_{23}^1 = i_{12} * f_{11}^1 + ...$$\n",
    "$$o_{32}^1 = i_{21} * f_{11}^1 + ...$$\n",
    "$$o_{33}^1 = i_{22} * f_{11}^1 + ...$$\n",
    "\n",
    "To update the filter, therefore, we should simply need to loop over the appropriate input image and output image locations for that filter.\n",
    "\n",
    "Now we add some complications:\n",
    "\n",
    "Channels\n",
    "\n",
    "What if we have channels in the input image? The input image could be three channels, for example. \n",
    "\n",
    "First, the forward pass. Each neuron in the output layer would now receive a sum from multiple filters: one slid over the “red” color channel of an image at a given location, one slid over the “blue” color channel, and one slid over a “green” color channel. \n",
    "\n",
    "[AlexNet example]\n",
    "\n",
    "How would having three channels modify the backward pass? The gradients for each image channel themselves wouldn’t change at all. \n",
    "\n",
    "What about the gradients for the filters? Those stay the same as well - except now we have 3 x 10 = 30 filters, for a total of 3 x 10 x 9 = 2700 weights. \n",
    "\n",
    "Batch size\n",
    "\n",
    "What if we are feeding multiple images into the network at once, instead of one at a time?\n",
    "\n",
    "The main place this changes things is that now the filters have more total influence on the output. For a given batch, the filters’ changes will be based on all the images in the input batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are neurons involved in pooling connected to future layers in the network?\n",
    "\n",
    "Consider four neurons that are combined into a single neuron via max pooling. The only neuron that actually influences the future layers in the network is the neuron that was chosen as the max. This neuron should simply be updated by whatever the gradient of the layer in front of it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding this up:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several things to code up here. We need a \n",
    "\n",
    "* Convolutional forward pass\n",
    "* Convolutional backward pass\n",
    "* Pooling forward pass\n",
    "* Pooling backward pass\n",
    "\n",
    "### Convolutional forward pass\n",
    "\n",
    "Should take in:\n",
    "\n",
    "* A 4d array of images: \n",
    "    * The first dimension will be the number of images (batch size)\n",
    "    * The second dimension will be the number of channels of this image\n",
    "    * The third and fourth dimensions will be the height and width of this image\n",
    "\n",
    "* A 4d array of filters:\n",
    "    * The first dimension will be the number of input channels\n",
    "    * The second dimension will be the number of output channels\n",
    "    * The third dimension will be the filter height\n",
    "    * The fourth dimension will be the filter width\n",
    "\n",
    "It returns:\n",
    "\n",
    "* A convout of shape:\n",
    "    * The first dimension will be the number of image (batch size)\n",
    "    * The second dimension will be the number of output channels\n",
    "    * The third dimension will be the output height (same as input height with \"same\" padding)\n",
    "    * The fourth dimension will be the output width (same as input width with \"same\" padding)\n",
    "\n",
    "### Convolutional backward pass\n",
    "\n",
    "Should take in:\n",
    "\n",
    "* The same images and filters of last time\n",
    "* A convout gradient with the same shape as the convout from the forward pass\n",
    "\n",
    "And return:\n",
    "\n",
    "* Filter gradients and image gradients of the same shape as the images and filters\n",
    "\n",
    "### Pooling forward pass\n",
    "\n",
    "Should take in:\n",
    "\n",
    "* The images - the same thing conceptually as a convout\n",
    "\n",
    "Should output:\n",
    "\n",
    "* \"poolout\": the convout but with the dimensions reduced due to the stride\n",
    "* An array saying which element was chosen as the maximum element (for max pooling)\n",
    "\n",
    "### Pooling backward pass\n",
    "\n",
    "Should take in:\n",
    "\n",
    "* The poolout gradient\n",
    "* The switches, saved from the forward pass\n",
    "\n",
    "Should output:\n",
    "\n",
    "* \"poolout\": the convout but with the dimensions reduced due to the stride\n",
    "* A gradient (3/4 of which will be zero) to pass back to the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implements aa convolutional forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bc01(imgs, filters, convout):\n",
    "    \"\"\" Multi-image, multi-channel convolution\n",
    "    imgs has shape (n_imgs, n_channels_in, img_h, img_w)\n",
    "    For the base neural net, this is: (32, 1, 28, 28)\n",
    "    filters has shape (n_channels_in, n_channels_out, img_h, img_w)\n",
    "    Foar the base neural net, this is: (1, 12, 5, 5)\n",
    "    convout has shape (n_imgs, n_channels_out, img_h, img_w)\n",
    "    For the base neural net, this is: (32, 12, 28, 28)\n",
    "    \"\"\"\n",
    "    # TODO: support padding and striding  \n",
    "    # TODO: experiment with border mode 'reflect'  \n",
    "\n",
    "    n_imgs = imgs.shape[0]\n",
    "    img_h = imgs.shape[2]\n",
    "    img_w = imgs.shape[3]\n",
    "    n_channels_in = filters.shape[0]\n",
    "    n_channels_out = filters.shape[1]\n",
    "    fil_h = filters.shape[2]\n",
    "    fil_w = filters.shape[3]\n",
    "    \n",
    "    fil_mid_h = fil_h // 2 # 3\n",
    "    fil_mid_w = fil_w // 2 # 3\n",
    "\n",
    "    for i in range(n_imgs): # batch size\n",
    "        for c_out in range(n_channels_out): # number of filters (e.g. 12)\n",
    "            for y in range(img_h): \n",
    "                y_off_min = max(-y, -fil_mid_h) # Get the minimum value of the filter\n",
    "                # If y = 1, fil_mid_h = 3, so max(-1, -3) = -1, so the \n",
    "                # filter will only go one unit in the y direction.\n",
    "                y_off_max = min(img_h-y, fil_mid_h+1)\n",
    "                # Similarly, if we are \"close to the top of the image\", \n",
    "                # cut off the filter height.\n",
    "                for x in range(img_w):\n",
    "                    x_off_min = max(-x, -fil_mid_w)\n",
    "                    # Similar logic for the left of the filter...\n",
    "                    x_off_max = min(img_w-x, fil_mid_w+1)\n",
    "                    # ...and the right of the filter.\n",
    "                    value = 0.0\n",
    "                    for y_off in range(y_off_min, y_off_max):\n",
    "                        for x_off in range(x_off_min, x_off_max):\n",
    "                            # Loop through the range of the filter, for:\n",
    "                            # A given image.\n",
    "                            # A given channel.\n",
    "                            # A given height and width value in the image.\n",
    "                            img_y = y + y_off\n",
    "                            img_x = x + x_off\n",
    "                            fil_y = fil_mid_w + y_off\n",
    "                            fil_x = fil_mid_h + x_off\n",
    "                            # Get the correct pixel value and the correct image value\n",
    "                            for c_in in range(n_channels_in): # For each channel into the image\n",
    "                                value += imgs[i, c_in, img_y, img_x] * filters[c_in, c_out, fil_y, fil_x]\n",
    "                                # Add the value in:\n",
    "                                # the first image, in the first pixel value, times the filter\n",
    "                    # For the value in the first pixel, first channel, if the convolution size for\n",
    "                    # this value is 3 x 3, then the value in cell (1,1) in the first output neuron is:\n",
    "                    # imgs[1, 1, 1, 1] * filters[1, 1, 1, 1] + \n",
    "                    # imgs[1, 1, 1, 2] * filters[1, 1, 1, 2] +\n",
    "                    # imgs[1, 1, 1, 3] * filters[1, 1, 1, 3] +\n",
    "                    # imgs[1, 1, 2, 1] * filters[1, 1, 2, 1] +\n",
    "                    # imgs[1, 1, 2, 2] * filters[1, 1, 2, 2] +\n",
    "                    # imgs[1, 1, 2, 3] * filters[1, 1, 2, 3] +\n",
    "                    # imgs[1, 1, 3, 1] * filters[1, 1, 3, 1] +\n",
    "                    # imgs[1, 1, 3, 2] * filters[1, 1, 3, 2] +\n",
    "                    # imgs[1, 1, 3, 3] * filters[1, 1, 3, 3]\n",
    "                    convout[i, c_out, y, x] = value    \n",
    "    \n",
    "    return convout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implements a convolutional backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bprop_conv_bc01(imgs, convout_grad, filters, imgs_grad, filters_grad):\n",
    "    \"\"\" Back-propagate gradients of multi-image, multi-channel convolution\n",
    "    Inputs:\n",
    "    imgs shape: (n_imgs, n_channels_in, img_h, img_w)\n",
    "    filters has shape (n_channels_in, n_channels_out, img_h, img_w)\n",
    "    convout_grad has same shape as convout: (n_imgs, n_channels_out, img_h, img_w)\n",
    "\n",
    "    Returns:\n",
    "    imgs_grad has same shape as imgs: (n_imgs, n_channels_in, img_h, img_w)\n",
    "    filters_grad has same shape as filters: (n_channels_in, n_channels_out, img_h, img_w)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    n_imgs = convout_grad.shape[0]\n",
    "    img_h = convout_grad.shape[2]\n",
    "    img_w = convout_grad.shape[3]\n",
    "    n_channels_convout = filters.shape[1]\n",
    "    n_channels_imgs = filters.shape[0]\n",
    "    fil_h = filters.shape[2]\n",
    "    fil_w = filters.shape[3]\n",
    "    fil_mid_h = fil_h // 2\n",
    "    fil_mid_w = fil_w // 2\n",
    "\n",
    "    imgs_grad = np.zeros((n_imgs, n_channels_imgs, img_h, img_w)) # Same shape as images\n",
    "    filters_grad = np.zeros((n_channels_imgs, n_channels_convout, fil_h, fil_w))  # Same shape as filters\n",
    "    for i in range(n_imgs):\n",
    "        for c_convout in range(n_channels_convout):\n",
    "            for y in range(img_h):\n",
    "                # Get the minimum and maximum indices of the convolutional filters.\n",
    "                y_off_min = max(-y, -fil_mid_h)\n",
    "                y_off_max = min(img_h-y, fil_mid_h+1)\n",
    "                for x in range(img_w):           \n",
    "                    convout_grad_value = convout_grad[i, c_convout, y, x]\n",
    "                    # Get the minimum and maximum indices of the convolutional filters.\n",
    "                    # indices: (image, channel_out, image \"y\", image \"x\")\n",
    "                    x_off_min = max(-x, -fil_mid_w)\n",
    "                    x_off_max = min(img_w-x, fil_mid_w+1)\n",
    "                    for y_off in range(y_off_min, y_off_max):\n",
    "                        for x_off in range(x_off_min, x_off_max):\n",
    "                            img_y = y + y_off\n",
    "                            img_x = x + x_off\n",
    "                            fil_y = fil_mid_w + y_off\n",
    "                            fil_x = fil_mid_h + x_off\n",
    "                            # n_channels_imgs = channels in (e.g. 3)\n",
    "                            for c_imgs in range(n_channels_imgs):\n",
    "                                # for each \"image\" channel:\n",
    "                                imgs_grad[i, c_imgs, img_y, img_x] += filters[c_imgs, c_convout, fil_y, fil_x] * convout_grad_value\n",
    "                                # Add to that value in the image gradient:\n",
    "                                # the sum of all the values from the filters from that particular image channel, times the gradient\n",
    "                                # for this convout layer\n",
    "                                filters_grad[c_imgs, c_convout, fil_y, fil_x] += imgs[i, c_imgs, img_y, img_x] * convout_grad_value\n",
    "                                # As for the filters gradient: add the values in the sum of all the images:\n",
    "                                # Add the sum of all the values *across all images* (as with regular gradient descent):\n",
    "                                # Of: all the values that were multiplied by that weight in the forward pass, \n",
    "                                # times the convout_grad_value\n",
    "    # Divide the filter_grad by the number of images\n",
    "    filters_grad /= n_imgs\n",
    "\n",
    "    \n",
    "    return imgs_grad, filters_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling: forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_bc01(imgs, poolout, switches, pool_h,\n",
    "              pool_w, stride_y, stride_x):\n",
    "    \"\"\" Multi-image, multi-channel pooling\n",
    "    imgs has shape (n_imgs, n_channels, img_h, img_w)\n",
    "    poolout has shape (n_imgs, n_channels, img_h//stride_y, img_w//stride_x)\n",
    "    switches has shape (n_imgs, n_channels, img_h//stride_y, img_w//stride_x, 2)\n",
    "    \"\"\"\n",
    "    # TODO: mean pool\n",
    "\n",
    "    n_imgs = imgs.shape[0]\n",
    "    n_channels = imgs.shape[1]\n",
    "    img_h = imgs.shape[2]\n",
    "    img_w = imgs.shape[3]\n",
    "\n",
    "    out_h = img_h // stride_y # 14\n",
    "    out_w = img_w // stride_x # 14\n",
    "\n",
    "    pool_h_top = pool_h // 2 - 1 + pool_h % 2 # 0 \n",
    "    pool_h_bottom = pool_h // 2 + 1 # 2\n",
    "    pool_w_left = pool_w // 2 - 1 + pool_w % 2 # 0\n",
    "    pool_w_right = pool_w // 2 + 1 # 2\n",
    "\n",
    "    if not n_imgs == poolout.shape[0] == switches.shape[0]:\n",
    "        raise ValueError('Mismatch in number of images.')\n",
    "    if not n_channels == poolout.shape[1] == switches.shape[1]:\n",
    "        raise ValueError('Mismatch in number of channels.')\n",
    "    if not (out_h == poolout.shape[2] == switches.shape[2] and out_w == poolout.shape[3] == switches.shape[3]):\n",
    "        raise ValueError('Mismatch in image shape.')\n",
    "    if not switches.shape[4] == 2:\n",
    "        raise ValueError('switches should only have length 2 in the 5. dimension.')\n",
    "\n",
    "    img_y_max = 0\n",
    "    img_x_max = 0\n",
    "\n",
    "    poolout = np.zeros((n_imgs, n_channels, out_h, out_w))\n",
    "    for i in range(n_imgs):\n",
    "        for c in range(n_channels):\n",
    "            for y_out in range(out_h):\n",
    "                y = y_out*stride_y # move along by stride_y\n",
    "                # min will either 0 or (usually) the bottom of the image area\n",
    "                y_min = max(y-pool_h_top, 0)\n",
    "                # max will either 28 or (usually) the top of the image area\n",
    "                y_max = min(y+pool_h_bottom, img_h)\n",
    "                # Calculate the same for x\n",
    "                for x_out in range(out_w):\n",
    "                    # move along by stride_x\n",
    "                    x = x_out*stride_x\n",
    "                    x_min = max(x-pool_w_left, 0)\n",
    "                    x_max = min(x+pool_w_right, img_w)\n",
    "                    value = -9e99\n",
    "                    for img_y in range(y_min, y_max):\n",
    "                        for img_x in range(x_min, x_max):\n",
    "                            # Within the correct image area: double for loop to find the max\n",
    "                            new_value = imgs[i, c, img_y, img_x]\n",
    "                            if new_value > value:\n",
    "                                value = new_value\n",
    "                                img_y_max = img_y\n",
    "                                img_x_max = img_x\n",
    "                    # Define poolout for this image and channel to be the max value\n",
    "                    poolout[i, c, y_out, x_out] = value\n",
    "                    # Define the switches to be the values that indices that contained the maxima\n",
    "                    # in each pool.\n",
    "                    switches[i, c, y_out, x_out, 0] = img_y_max\n",
    "                    switches[i, c, y_out, x_out, 1] = img_x_max\n",
    "                    \n",
    "    return poolout, switches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling: backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bprop_pool_bc01(poolout_grad, switches, imgs_grad):\n",
    "    \"\"\" Multi-image, multi-channel pooling\n",
    "    imgs_grad has same shape as imgs (n_imgs, n_channels, img_h, img_w)\n",
    "    poolout_grad has same shape as poolout: (n_imgs, n_channels, img_h//stride_y, img_w//stride_x)\n",
    "    switches has shape (n_imgs, n_channels, img_h//stride_y, img_w//stride_x, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    # \"poolout\" = \"*Output* of pooling\n",
    "    n_imgs = poolout_grad.shape[0] # 32\n",
    "    n_channels = poolout_grad.shape[1] # 12 *number of channels of the layer \"prior\" to pooling*\n",
    "    poolout_h = poolout_grad.shape[2] # 14\n",
    "    poolout_w = poolout_grad.shape[3] # 14\n",
    "\n",
    "#     imgs_grad = np.zeros((n_imgs, n_channels, imgs_grad.shape[2], imgs_grad.shape[3]))\n",
    "    # For each image\n",
    "    for i in range(n_imgs):\n",
    "        # For each channel \n",
    "        for c in range(n_channels):\n",
    "            for y in range(poolout_h):\n",
    "                for x in range(poolout_w):\n",
    "                    # Double for loop over 14 x 14\n",
    "                    # Get the indices in the prior image that contained the max values.\n",
    "                    img_y = switches[i, c, y, x, 0]\n",
    "                    img_x = switches[i, c, y, x, 1]\n",
    "                    # Make the gradient of those pixels in the images equal to\n",
    "                    # the poolout gradient (the output gradient of the pooling \n",
    "                    # layer at those pixel values.\n",
    "                    imgs_grad[i, c, img_y, img_x] = poolout_grad[i, c, y, x]\n",
    "    return imgs_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be a look inside convolutional neural networks. We'll be building them from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_net import conv_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello convolution!\n"
     ]
    }
   ],
   "source": [
    "conv_helpers.hello_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(Layer, ParamMixin):\n",
    "    def __init__(self, n_feats, filter_shape, strides, weight_scale,\n",
    "                 weight_decay=0.0, padding_mode='same', border_mode='nearest'):\n",
    "        self.n_feats = n_feats\n",
    "        self.filter_shape = filter_shape\n",
    "        self.strides = strides\n",
    "        self.weight_scale = weight_scale\n",
    "        self.weight_decay = weight_decay\n",
    "        self.padding_mode = padding_mode\n",
    "        self.border_mode = border_mode\n",
    "\n",
    "    def _setup(self, input_shape, rng):\n",
    "        n_channels = input_shape[1]\n",
    "        W_shape = (n_channels, self.n_feats) + self.filter_shape\n",
    "        self.W = rng.normal(size=W_shape, scale=self.weight_scale)\n",
    "        self.b = np.zeros(self.n_feats)\n",
    "\n",
    "    def fprop(self, input):\n",
    "        self.last_input = input\n",
    "        self.last_input_shape = input.shape\n",
    "        convout = np.empty(self.output_shape(input.shape))\n",
    "        convout = conv_bc01(input, self.W, convout)\n",
    "        return convout + self.b[np.newaxis, :, np.newaxis, np.newaxis]\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        input_grad = np.empty(self.last_input_shape)\n",
    "        self.dW = np.empty(self.W.shape)\n",
    "        input_grad, self.dW = bprop_conv_bc01(self.last_input, output_grad,\n",
    "                                              self.W, input_grad, self.dW)\n",
    "        n_imgs = output_grad.shape[0]\n",
    "        self.db = np.sum(output_grad, axis=(0, 2, 3)) / (n_imgs)\n",
    "        self.dW -= self.weight_decay*self.W\n",
    "        return input_grad\n",
    "\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    def param_incs(self):\n",
    "        return self.dW, self.db\n",
    "\n",
    "    def param_grads(self):\n",
    "        # undo weight decay\n",
    "        gW = self.dW+self.weight_decay*self.W\n",
    "        return gW, self.db\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        if self.padding_mode == 'same':\n",
    "            h = input_shape[2]\n",
    "            w = input_shape[3]\n",
    "        elif self.padding_mode == 'full':\n",
    "            h = input_shape[2]-self.filter_shape[1]+1\n",
    "            w = input_shape[3]-self.filter_shape[2]+1\n",
    "        else:\n",
    "            h = input_shape[2]+self.filter_shape[1]-1\n",
    "            w = input_shape[3]+self.filter_shape[2]-1\n",
    "        shape = (input_shape[0], self.n_feats, h, w)\n",
    "        return shape\n",
    "\n",
    "\n",
    "class Pool(Layer):\n",
    "    def __init__(self, pool_shape=(3, 3), strides=(1, 1), mode='max'):\n",
    "        self.mode = mode\n",
    "        self.pool_h, self.pool_w = pool_shape\n",
    "        self.stride_y, self.stride_x = strides\n",
    "\n",
    "    def fprop(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        self.last_switches = np.empty(self.output_shape(input.shape)+(2,),\n",
    "                                      dtype=np.int)\n",
    "        poolout = np.empty(self.output_shape(input.shape))\n",
    "        poolout, switches = pool_bc01(input, poolout, self.last_switches, self.pool_h, self.pool_w,\n",
    "                  self.stride_y, self.stride_x)\n",
    "        self.last_switches = switches\n",
    "        return poolout\n",
    "\n",
    "    def bprop(self, output_grad):\n",
    "        input_grad = np.empty(self.last_input_shape)\n",
    "        input_grad = bprop_pool_bc01(output_grad, self.last_switches, input_grad)\n",
    "        return input_grad\n",
    "\n",
    "    def output_shape(self, input_shape):\n",
    "        shape = (input_shape[0],\n",
    "                 input_shape[1],\n",
    "                 input_shape[2]//self.stride_y,\n",
    "                 input_shape[3]//self.stride_x)\n",
    "        return shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bprop_pool_bc01(poolout_grad, switches, imgs_grad):\n",
    "    \"\"\" Multi-image, multi-channel pooling\n",
    "    imgs_grad has same shape as imgs (n_imgs, n_channels, img_h, img_w)\n",
    "    poolout_grad has same shape as poolout: (n_imgs, n_channels, img_h//stride_y, img_w//stride_x)\n",
    "    switches has shape (n_imgs, n_channels, img_h//stride_y, img_w//stride_x, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    # \"poolout\" = \"*Output* of pooling\n",
    "    n_imgs = poolout_grad.shape[0] # 32\n",
    "    n_channels = poolout_grad.shape[1] # 12 *number of channels of the layer \"prior\" to pooling*\n",
    "    poolout_h = poolout_grad.shape[2] # 14\n",
    "    poolout_w = poolout_grad.shape[3] # 14\n",
    "\n",
    "#     imgs_grad = np.zeros((n_imgs, n_channels, imgs_grad.shape[2], imgs_grad.shape[3]))\n",
    "    # For each image\n",
    "    for i in range(n_imgs):\n",
    "        # For each channel \n",
    "        for c in range(n_channels):\n",
    "            for y in range(poolout_h):\n",
    "                for x in range(poolout_w):\n",
    "                    # Double for loop over 14 x 14\n",
    "                    # Get the indices in the prior image that contained the max values.\n",
    "                    img_y = switches[i, c, y, x, 0]\n",
    "                    img_x = switches[i, c, y, x, 1]\n",
    "                    # Make the gradient of those pixels in the images equal to\n",
    "                    # the poolout gradient (the output gradient of the pooling \n",
    "                    # layer at those pixel values.\n",
    "                    imgs_grad[i, c, img_y, img_x] = poolout_grad[i, c, y, x]\n",
    "    return imgs_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are layers in a convolutional neural network connected?\n",
    "\n",
    "If we start off with an image that is 32 x 32 x 3 - color images - how can we change this to a convolutional layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've all see diagrams like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the convolution actually happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A step back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, neural networks are a series of neurons that are connected to each other. Each neuron receives input from many neurons, transforms it in some way, and sends that input forward to many neurons.\n",
    "\n",
    "It is important when reasoning about how neural nets are constructed that we distinguish between the neurons themselves, which receive inputs and are units of computation, and the weights, which represent connections between the neurons and are what changes over time as a neural net \"learns\". \n",
    "\n",
    "In classic convolutional neural net diagrams, what we are in fact looking at is the _neurons_ themselves, not the weights. The question then becomes:\n",
    "\n",
    "1. How are the layers actually connected - what is connected to what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An image starts out as a 3 channel thing: red, green, and blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happens in the convolution\n",
    "\n",
    "One filter is slid just over one channel in the input, and connects to one channel in the output.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider an individual 3 x 3 filter that is slid over an image. How will this filter be connected to the next layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two parts to the convolution explanation. \n",
    "\n",
    "- The first is having a visual understanding of how everything is connected\n",
    "- The second is figuring out how to write the for loops to make it all happen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top left corner of the filter will be multiplied by the appropriate range of pixels in the output image (perhaps excluding the bottom right). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, if the input has three channels, the filter will be multiplied by each channel of each part of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient that the filters will receive is of shape:\n",
    "\n",
    "[1, 12, 28, 28]\n",
    "\n",
    "Essentially, 12 channels.\n",
    "\n",
    "The images being multiplied by these filters are of shape:\n",
    "\n",
    "[1, 1 (of 3), 28, 28]\n",
    "\n",
    "For the filters, the element:\n",
    "\n",
    "[1 (of 3), 1 (of 12), 1, 1]\n",
    "\n",
    "contributes to certain outputs in the next layer that are all the places where a pixel in the image was multiplied by a this particular filter value to get a particular value in the output. For image size of 28x28 and stride of 5, this would correspond to roughly a 24 x 24 patch. \n",
    "\n",
    "So, _the sum takes place for a given input channel and output channel_, across all the image locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For images, let's consider a pixel in input channel 1, and where affects the next layer. It is going to affect all 12 of the filters - and again, affect them based on the components of the filter that it has been multiplied by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image-filter[0,0] combination would map to one location in the next layer, image-filter[0,1] would map to a different location etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An insight: even though the channels differ in the convout, a given filter location and image location maps to one particular convout location. \n",
    "\n",
    "The converse is not true: a given convout location is mapped to by several filter-image location combinations. \n",
    "\n",
    "So, the overall strategy will be:\n",
    "1. Fix a convout location (including the channel_out)\n",
    "2. Find all the filter x and y locations that map to that (will usually be all, unless we're on an edge)\n",
    "3. Find the image locations that correspond to that convout location (these should correspond to the filter locations)\n",
    "4. Update the _filter gradient_ by looping over the _image locations_ that map to this convout location.\n",
    "5. Update the _image gradient_ by looping over the _filter locations_ that map to this convout location. \n",
    "6. Do steps 4 and 5 for each input image channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, getting the filter gradient, and the image gradient, and the convout gradient to line up correctly involves looping over:\n",
    "1. Fix a location in the convout:\n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
